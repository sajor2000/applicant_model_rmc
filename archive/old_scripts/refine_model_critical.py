"""
Critical Model Refinement - Focus on Key Improvements
====================================================

Streamlined refinement focusing on the most impactful changes to reduce
low confidence predictions.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import joblib
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')


class CriticalCascadeOptimizer:
    """Critical improvements only - fast execution."""
    
    def __init__(self):
        self.best_models = {}
        self.calibrators = {}
        
    def load_training_data(self):
        """Load and prepare training data."""
        print("Loading training data...")
        
        # Load filtered data
        df_2022 = pd.read_excel("data_filtered/2022_filtered_applicants.xlsx")
        df_2023 = pd.read_excel("data_filtered/2023_filtered_applicants.xlsx")
        
        # Load LLM scores
        llm_train = pd.read_csv("llm_scores_2022_2023_20250619_172837.csv")
        llm_train = llm_train.rename(columns={'AMCAS_ID_original': 'amcas_id'})
        
        # Merge
        llm_cols = [col for col in llm_train.columns if col.startswith('llm_')]
        df_2022 = df_2022.merge(llm_train[['amcas_id'] + llm_cols], on='amcas_id', how='left')
        df_2023 = df_2023.merge(llm_train[['amcas_id'] + llm_cols], on='amcas_id', how='left')
        
        df_train = pd.concat([df_2022, df_2023], ignore_index=True)
        print(f"Total training samples: {len(df_train)}")
        
        return df_train
    
    def engineer_critical_features(self, df):
        """Create only the most critical features."""
        print("Engineering critical features...")
        
        df = df.copy()
        
        # Essay-service alignment (highest impact)
        if 'llm_overall_essay_score' in df.columns and 'service_rating_numerical' in df.columns:
            essay_norm = df['llm_overall_essay_score'] / 100
            service_norm = (df['service_rating_numerical'] - 1) / 3
            df['essay_service_alignment'] = 1 - abs(essay_norm - service_norm)
            df['service_essay_product'] = df['service_rating_numerical'] * df['llm_overall_essay_score'] / 25
        
        # Flag balance
        if 'llm_red_flag_count' in df.columns and 'llm_green_flag_count' in df.columns:
            df['flag_balance'] = df['llm_green_flag_count'] - df['llm_red_flag_count']
        
        # Service Ã— Clinical interaction
        if 'service_rating_numerical' in df.columns and 'healthcare_total_hours' in df.columns:
            df['service_clinical_log'] = df['service_rating_numerical'] * np.log1p(df['healthcare_total_hours'])
        
        return df
    
    def prepare_data(self, df):
        """Prepare data for training."""
        # Get features and target
        exclude_cols = ['application_review_score', 'amcas_id', 'appl_year', 'year', 'AMCAS_ID']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        X = df[feature_cols].copy()
        y = df['application_review_score'].values
        
        # Handle categorical features
        categorical_mappings = {
            'gender': {'Male': 0, 'Female': 1, 'Other': 2, 'Unknown': -1},
            'citizenship': {'US_Citizen': 0, 'Permanent_Resident': 1, 'International': 2, 'Other': 3},
            'service_rating_categorical': {
                'Exceptional': 5, 'Outstanding': 4, 'Excellent': 3,
                'Good': 2, 'Average': 1, 'Below Average': 0, 'Poor': -1
            }
        }
        
        for col, mapping in categorical_mappings.items():
            if col in X.columns:
                X[col] = X[col].map(mapping).fillna(-2)
        
        # Convert remaining categoricals
        for col in X.select_dtypes(include=['object']).columns:
            X[col] = pd.Categorical(X[col]).codes
        
        # Impute and scale
        imputer = SimpleImputer(strategy='median')
        X_imputed = imputer.fit_transform(X)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_imputed)
        
        return X_scaled, y, feature_cols, imputer, scaler
    
    def train_ensemble_stage(self, X, y, stage_name):
        """Train ensemble with optimal fixed parameters."""
        print(f"\nTraining ensemble for {stage_name}...")
        
        # Calculate class weight
        pos_weight = (y == 0).sum() / (y == 1).sum() if (y == 1).sum() > 0 else 1
        
        # Fixed optimal parameters based on previous runs
        if stage_name == "Stage 1":
            params = {
                'n_estimators': 700,
                'max_depth': 4,
                'learning_rate': 0.02,
                'min_child_weight': 3,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.5,
                'reg_lambda': 2,
                'scale_pos_weight': pos_weight
            }
        elif stage_name == "Stage 2":
            params = {
                'n_estimators': 600,
                'max_depth': 5,
                'learning_rate': 0.02,
                'min_child_weight': 5,
                'subsample': 0.8,
                'colsample_bytree': 0.7,
                'reg_alpha': 1,
                'reg_lambda': 2,
                'scale_pos_weight': pos_weight
            }
        else:  # Stage 3
            params = {
                'n_estimators': 800,
                'max_depth': 6,
                'learning_rate': 0.05,
                'min_child_weight': 1,
                'subsample': 0.9,
                'colsample_bytree': 0.9,
                'reg_alpha': 1,
                'reg_lambda': 1,
                'scale_pos_weight': pos_weight
            }
        
        # Common parameters
        params.update({
            'random_state': 42,
            'use_label_encoder': False,
            'eval_metric': 'logloss',
            'tree_method': 'hist'
        })
        
        # Train 3 models with different seeds
        models = []
        for i in range(3):
            model_params = params.copy()
            model_params['random_state'] = i * 42
            model = XGBClassifier(**model_params)
            model.fit(X, y)
            models.append(model)
            print(f"  Model {i+1}/3 trained")
        
        # Create voting ensemble
        ensemble = VotingClassifier(
            estimators=[(f'xgb_{i}', model) for i, model in enumerate(models)],
            voting='soft'
        )
        ensemble.fit(X, y)
        
        # Calibrate probabilities
        calibrated = CalibratedClassifierCV(ensemble, method='isotonic', cv=3)
        calibrated.fit(X, y)
        
        return calibrated
    
    def train_cascade(self, df_train):
        """Train the cascade classifier."""
        print("\n" + "="*80)
        print("TRAINING CRITICAL CASCADE CLASSIFIER")
        print("="*80)
        
        # Engineer features
        df_enhanced = self.engineer_critical_features(df_train)
        
        # Prepare data
        X, y, feature_cols, imputer, scaler = self.prepare_data(df_enhanced)
        
        # Stage 1: Reject vs Non-Reject
        print("\n### STAGE 1: Reject vs Non-Reject ###")
        y_stage1 = (y > 9).astype(int)
        stage1_model = self.train_ensemble_stage(X, y_stage1, "Stage 1")
        self.best_models['stage1'] = stage1_model
        
        # Stage 2: Waitlist vs Higher
        print("\n### STAGE 2: Waitlist vs Higher ###")
        mask_stage2 = y > 9
        X_stage2 = X[mask_stage2]
        y_stage2 = (y[mask_stage2] > 15).astype(int)
        stage2_model = self.train_ensemble_stage(X_stage2, y_stage2, "Stage 2")
        self.best_models['stage2'] = stage2_model
        
        # Stage 3: Interview vs Accept
        print("\n### STAGE 3: Interview vs Accept ###")
        mask_stage3 = y > 15
        X_stage3 = X[mask_stage3]
        y_stage3 = (y[mask_stage3] >= 23).astype(int)
        stage3_model = self.train_ensemble_stage(X_stage3, y_stage3, "Stage 3")
        self.best_models['stage3'] = stage3_model
        
        return feature_cols, imputer, scaler
    
    def predict_with_improved_confidence(self, X):
        """Make predictions with improved confidence calculation."""
        n_samples = X.shape[0]
        predictions = np.zeros(n_samples, dtype=int)
        probabilities = np.zeros((n_samples, 4))
        confidences = np.zeros(n_samples)
        
        # Stage 1
        stage1_proba = self.best_models['stage1'].predict_proba(X)
        prob_not_reject = stage1_proba[:, 1]
        
        # Improved confidence: use entropy-based measure
        entropy = -prob_not_reject * np.log(prob_not_reject + 1e-10) - (1 - prob_not_reject) * np.log(1 - prob_not_reject + 1e-10)
        stage1_confidence = 1 - entropy / np.log(2)  # Normalize to 0-1
        
        # Reject decisions
        is_reject = prob_not_reject < 0.5
        predictions[is_reject] = 0
        probabilities[is_reject, 0] = 1 - prob_not_reject[is_reject]
        confidences[is_reject] = stage1_confidence[is_reject]
        
        # Non-reject
        non_reject_mask = ~is_reject
        if non_reject_mask.sum() > 0:
            X_stage2 = X[non_reject_mask]
            
            # Stage 2
            stage2_proba = self.best_models['stage2'].predict_proba(X_stage2)
            prob_higher = stage2_proba[:, 1]
            
            entropy2 = -prob_higher * np.log(prob_higher + 1e-10) - (1 - prob_higher) * np.log(1 - prob_higher + 1e-10)
            stage2_confidence = 1 - entropy2 / np.log(2)
            
            # Waitlist
            is_waitlist = prob_higher < 0.5
            waitlist_indices = np.where(non_reject_mask)[0][is_waitlist]
            predictions[waitlist_indices] = 1
            probabilities[waitlist_indices, 1] = 1 - prob_higher[is_waitlist]
            
            # Combined confidence using minimum (weakest link)
            non_reject_conf = confidences[non_reject_mask]
            non_reject_conf[is_waitlist] = np.minimum(
                stage1_confidence[non_reject_mask][is_waitlist],
                stage2_confidence[is_waitlist]
            )
            confidences[non_reject_mask] = non_reject_conf
            
            # Higher
            higher_mask = np.zeros(n_samples, dtype=bool)
            higher_indices = np.where(non_reject_mask)[0][~is_waitlist]
            higher_mask[higher_indices] = True
            
            if higher_mask.sum() > 0:
                X_stage3 = X[higher_mask]
                
                # Stage 3
                stage3_proba = self.best_models['stage3'].predict_proba(X_stage3)
                prob_accept = stage3_proba[:, 1]
                
                entropy3 = -prob_accept * np.log(prob_accept + 1e-10) - (1 - prob_accept) * np.log(1 - prob_accept + 1e-10)
                stage3_confidence = 1 - entropy3 / np.log(2)
                
                # Accept/Interview
                is_accept = prob_accept >= 0.5
                accept_indices = higher_indices[is_accept]
                interview_indices = higher_indices[~is_accept]
                
                predictions[accept_indices] = 3
                predictions[interview_indices] = 2
                
                probabilities[accept_indices, 3] = prob_accept[is_accept]
                probabilities[interview_indices, 2] = 1 - prob_accept[~is_accept]
                
                # Final confidence (minimum of all stages)
                higher_conf = confidences[higher_mask]
                stage1_conf_higher = stage1_confidence[higher_mask]
                stage2_conf_higher = stage2_confidence[~is_waitlist]
                
                higher_conf = np.minimum(
                    np.minimum(stage1_conf_higher, stage2_conf_higher),
                    stage3_confidence
                )
                confidences[higher_mask] = higher_conf
        
        # Fill remaining probabilities
        for i in range(n_samples):
            prob_sum = probabilities[i].sum()
            if prob_sum < 0.99:
                probabilities[i, predictions[i]] = 1 - prob_sum
        
        # Scale confidence to 0-100 with better distribution
        # Use power transformation to spread out confidence scores
        confidences = 100 * (confidences ** 0.7)
        
        return predictions, probabilities, confidences
    
    def evaluate_on_test(self, df_test, feature_cols, imputer, scaler):
        """Evaluate model on test data."""
        print("\n" + "="*80)
        print("EVALUATING ON 2024 TEST DATA")
        print("="*80)
        
        # Engineer same features
        df_test_enhanced = self.engineer_critical_features(df_test)
        
        # Prepare test data
        X_test, y_test, _, _, _ = self.prepare_data(df_test_enhanced)
        
        # Predict
        predictions, probabilities, confidences = self.predict_with_improved_confidence(X_test)
        
        # Convert to buckets
        true_buckets = np.zeros(len(y_test), dtype=int)
        true_buckets[y_test <= 9] = 0
        true_buckets[(y_test >= 10) & (y_test <= 15)] = 1
        true_buckets[(y_test >= 16) & (y_test <= 22)] = 2
        true_buckets[y_test >= 23] = 3
        
        # Calculate metrics
        exact_match = np.mean(predictions == true_buckets)
        adjacent = np.mean(np.abs(predictions - true_buckets) <= 1)
        
        # Confidence distribution
        low_conf = (confidences < 60).sum()
        med_conf = ((confidences >= 60) & (confidences < 80)).sum()
        high_conf = (confidences >= 80).sum()
        
        print(f"\nPerformance Metrics:")
        print(f"Exact Match: {exact_match:.3f}")
        print(f"Adjacent Accuracy: {adjacent:.3f}")
        
        print(f"\nConfidence Distribution:")
        print(f"Low (<60): {low_conf} ({low_conf/len(confidences)*100:.1f}%)")
        print(f"Medium (60-80): {med_conf} ({med_conf/len(confidences)*100:.1f}%)")
        print(f"High (>80): {high_conf} ({high_conf/len(confidences)*100:.1f}%)")
        
        # Analyze low confidence cases
        low_conf_mask = confidences < 60
        if low_conf_mask.sum() > 0:
            print(f"\nLow Confidence Analysis:")
            print(f"Borderline Q1/Q2: {((predictions[low_conf_mask] == 1) | (predictions[low_conf_mask] == 2)).sum()}")
            print(f"Borderline Q3/Q4: {((predictions[low_conf_mask] == 0) | (predictions[low_conf_mask] == 1)).sum()}")
            
            # Check probability distributions
            max_probs = probabilities[low_conf_mask].max(axis=1)
            print(f"Average max probability: {max_probs.mean():.3f}")
        
        # Confusion matrix
        cm = confusion_matrix(true_buckets, predictions)
        bucket_names = ['Reject', 'Waitlist', 'Interview', 'Accept']
        
        print("\nConfusion Matrix:")
        print("True\\Pred   " + "  ".join(f"{name:>10}" for name in bucket_names))
        for i, true_name in enumerate(bucket_names):
            row = f"{true_name:10} "
            for j in range(4):
                row += f"{cm[i,j]:>10} "
            print(row)
        
        return predictions, confidences, probabilities, exact_match, adjacent


def main():
    # Initialize optimizer
    optimizer = CriticalCascadeOptimizer()
    
    # Load training data
    df_train = optimizer.load_training_data()
    
    # Train cascade
    feature_cols, imputer, scaler = optimizer.train_cascade(df_train)
    
    # Load test data
    df_test = pd.read_excel("data_filtered/2024_filtered_applicants.xlsx")
    llm_test_files = list(Path(".").glob("llm_scores_2024_*.csv"))
    if llm_test_files:
        llm_test = pd.read_csv(sorted(llm_test_files)[-1])
        llm_test = llm_test.rename(columns={'AMCAS_ID_original': 'amcas_id'})
        llm_cols = [col for col in llm_test.columns if col.startswith('llm_')]
        df_test = df_test.merge(llm_test[['amcas_id'] + llm_cols], on='amcas_id', how='left')
    
    # Evaluate
    predictions, confidences, probabilities, exact_match, adjacent = optimizer.evaluate_on_test(
        df_test, feature_cols, imputer, scaler
    )
    
    # Save model
    print("\nSaving optimized model...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    model_data = {
        'optimizer': optimizer,
        'feature_cols': feature_cols,
        'imputer': imputer,
        'scaler': scaler,
        'performance': {
            'exact_match': exact_match,
            'adjacent': adjacent
        }
    }
    
    model_path = f"models/critical_cascade_{timestamp}.pkl"
    joblib.dump(model_data, model_path)
    print(f"Model saved to: {model_path}")
    
    # Save predictions
    results_df = pd.DataFrame({
        'amcas_id': df_test['amcas_id'],
        'true_score': df_test['application_review_score'],
        'predicted_bucket': predictions,
        'confidence': confidences
    })
    
    results_path = f"critical_predictions_{timestamp}.csv"
    results_df.to_csv(results_path, index=False)
    print(f"Predictions saved to: {results_path}")
    
    print("\n" + "="*80)
    print("CRITICAL OPTIMIZATION COMPLETE!")
    print("="*80)


if __name__ == "__main__":
    main()